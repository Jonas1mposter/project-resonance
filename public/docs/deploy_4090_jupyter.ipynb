{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# ğŸ™ï¸ Project Resonance â€” RTX 4090 JupyterLab ä¸€é”®éƒ¨ç½²\n",
    "\n",
    "> **é€‚ç”¨ç¯å¢ƒ**ï¼šJupyterLab | NVIDIA RTX 4090 (24GB VRAM) | CUDA 12.x/13.x  \n",
    "> **ä½¿ç”¨æ–¹å¼**ï¼šä»ä¸Šåˆ°ä¸‹é€ä¸ª Cell è¿è¡Œï¼ˆ`Shift + Enter`ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "| æ­¥éª¤ | å†…å®¹ | é¢„è®¡æ—¶é—´ |\n",
    "|------|------|----------|\n",
    "| 1 | æ£€æŸ¥ GPU ç¯å¢ƒ | < 1 åˆ†é’Ÿ |\n",
    "| 2 | å®‰è£…ä¾èµ– | 3-5 åˆ†é’Ÿ |\n",
    "| 3 | é…ç½®è®­ç»ƒå‚æ•° | < 1 åˆ†é’Ÿ |\n",
    "| 4 | è®­ç»ƒæ¨¡å‹ | 15-20 åˆ†é’Ÿ |\n",
    "| 5 | å¯åŠ¨æ¨ç†æœåŠ¡ | < 1 åˆ†é’Ÿ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-step1-title",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 1 / 5 â€” æ£€æŸ¥ GPU ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-step1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "def check_gpu():\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=name,memory.total,driver_version',\n",
    "             '--format=csv,noheader'],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "        for i, line in enumerate(lines):\n",
    "            parts = [p.strip() for p in line.split(',')]\n",
    "            gpu_name, mem, driver = parts[0], parts[1], parts[2]\n",
    "            mem_mb = int(mem.replace(' MiB', ''))\n",
    "            print(f'  GPU #{i}:  {gpu_name}')\n",
    "            print(f'  æ˜¾å­˜:    {mem}')\n",
    "            print(f'  é©±åŠ¨:    {driver}')\n",
    "            if mem_mb >= 20000:\n",
    "                print(f'  âœ… æ˜¾å­˜å……è¶³ï¼Œå¯ä½¿ç”¨ batch_size=32')\n",
    "            else:\n",
    "                print(f'  âš ï¸  æ˜¾å­˜ä¸è¶³ 20GBï¼Œå»ºè®®å°† BATCH_SIZE æ”¹ä¸º 8')\n",
    "    except FileNotFoundError:\n",
    "        print('âŒ æœªæ‰¾åˆ° nvidia-smiï¼Œè¯·ç¡®è®¤ GPU é©±åŠ¨å·²å®‰è£…')\n",
    "        sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        import torch\n",
    "        print(f'\\n  PyTorch:  {torch.__version__}')\n",
    "        print(f'  CUDA:     {torch.version.cuda}')\n",
    "        print(f'  BF16:     {\"âœ… æ”¯æŒ\" if torch.cuda.is_bf16_supported() else \"âŒ ä¸æ”¯æŒï¼ˆå°†å›é€€ FP16ï¼‰\"}')\n",
    "    except ImportError:\n",
    "        print('  âš ï¸  PyTorch å°šæœªå®‰è£…ï¼Œè¯·å…ˆè¿è¡Œæ­¥éª¤ 2')\n",
    "\n",
    "print('=' * 50)\n",
    "print('  GPU ç¯å¢ƒæ£€æµ‹')\n",
    "print('=' * 50)\n",
    "check_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-step2-title",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 2 / 5 â€” å®‰è£… Python ä¾èµ–\n",
    "\n",
    "> â³ é¦–æ¬¡è¿è¡Œçº¦ 3-5 åˆ†é’Ÿï¼Œå·²å®‰è£…åˆ™å‡ ç§’å®Œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-step2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "packages = [\n",
    "    'transformers',\n",
    "    'peft',\n",
    "    'datasets',\n",
    "    'soundfile',\n",
    "    'librosa',\n",
    "    'evaluate',\n",
    "    'jiwer',\n",
    "    'fastapi',\n",
    "    'uvicorn[standard]',\n",
    "    'python-multipart',\n",
    "    'accelerate',\n",
    "    'nest-asyncio',   # JupyterLab å¼‚æ­¥å…¼å®¹å¿…é¡»\n",
    "]\n",
    "\n",
    "print('ğŸ“¦ æ­£åœ¨å®‰è£…ä¾èµ–...')\n",
    "result = subprocess.run(\n",
    "    [sys.executable, '-m', 'pip', 'install', '-q'] + packages,\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print('âœ… ä¾èµ–å®‰è£…å®Œæˆ')\n",
    "    print('\\nå·²å®‰è£…åŒ…ï¼š')\n",
    "    for p in packages:\n",
    "        print(f'  â€¢ {p}')\n",
    "else:\n",
    "    print('âŒ å®‰è£…å¤±è´¥ï¼š')\n",
    "    print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-step3-title",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 3 / 5 â€” é…ç½®è®­ç»ƒå‚æ•°\n",
    "\n",
    "> âœï¸ **åœ¨æ­¤ä¿®æ”¹è·¯å¾„å’Œå‚æ•°**ï¼Œç„¶åè¿è¡Œæ­¤ Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-step3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# â”€â”€ âœï¸ æŒ‰éœ€ä¿®æ”¹ä»¥ä¸‹é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DATA_DIR    = '/root/data'            # è®­ç»ƒæ•°æ®ç›®å½•ï¼ˆå« labels.json + WAV æ–‡ä»¶ï¼‰\n",
    "MODEL_DIR   = '/root/lora_output'     # LoRA æƒé‡ä¿å­˜ç›®å½•\n",
    "BASE_MODEL  = 'openai/whisper-small'  # åŸºç¡€æ¨¡å‹ï¼ˆsmall/medium/large-v3ï¼‰\n",
    "LANGUAGE    = 'zh'                    # ç›®æ ‡è¯­è¨€\n",
    "TRAIN_STEPS = 500                     # è®­ç»ƒæ­¥æ•°ï¼ˆæ ·æœ¬å°‘å¯é™è‡³ 200ï¼‰\n",
    "BATCH_SIZE  = 32                      # RTX 4090 æ¨è 32ï¼›æ˜¾å­˜ä¸è¶³æ”¹ä¸º 8\n",
    "SERVER_PORT = 8000                    # æ¨ç†æœåŠ¡ç«¯å£\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# è‡ªåŠ¨æ£€æµ‹æ•°æ®å¹¶å†³å®šæ˜¯å¦è·³è¿‡è®­ç»ƒ\n",
    "labels_path = os.path.join(DATA_DIR, 'labels.json')\n",
    "SKIP_TRAINING = False\n",
    "RECORD_COUNT  = 0\n",
    "\n",
    "print('ğŸ“‹ å½“å‰é…ç½®ï¼š')\n",
    "print(f'  æ•°æ®ç›®å½•:  {DATA_DIR}')\n",
    "print(f'  æ¨¡å‹ç›®å½•:  {MODEL_DIR}')\n",
    "print(f'  åŸºç¡€æ¨¡å‹:  {BASE_MODEL}')\n",
    "print(f'  è®­ç»ƒæ­¥æ•°:  {TRAIN_STEPS}')\n",
    "print(f'  Batch å¤§å°: {BATCH_SIZE}')\n",
    "print(f'  æœåŠ¡ç«¯å£:  {SERVER_PORT}')\n",
    "\n",
    "if not os.path.isdir(DATA_DIR):\n",
    "    print(f'\\nâš ï¸  æ•°æ®ç›®å½• {DATA_DIR} ä¸å­˜åœ¨ â†’ è·³è¿‡è®­ç»ƒï¼Œä»…éƒ¨ç½²æ¨ç†æœåŠ¡')\n",
    "    SKIP_TRAINING = True\n",
    "elif not os.path.isfile(labels_path):\n",
    "    print(f'\\nâš ï¸  æœªæ‰¾åˆ° {labels_path} â†’ è·³è¿‡è®­ç»ƒ')\n",
    "    SKIP_TRAINING = True\n",
    "else:\n",
    "    import json\n",
    "    with open(labels_path) as f:\n",
    "        RECORD_COUNT = len(json.load(f))\n",
    "    print(f'\\nâœ… æ‰¾åˆ° {RECORD_COUNT} æ¡è®­ç»ƒæ ·æœ¬')\n",
    "    if RECORD_COUNT < 10:\n",
    "        print('  âš ï¸  æ ·æœ¬å°‘äº 10 æ¡ï¼Œå»ºè®®è‡³å°‘å‡†å¤‡ 50 æ¡ä»¥è·å¾—è¾ƒå¥½æ•ˆæœ')\n",
    "\n",
    "if os.path.isfile(os.path.join(MODEL_DIR, 'adapter_config.json')):\n",
    "    print(f'\\nâœ… å‘ç°å·²æœ‰ LoRA æƒé‡ï¼ˆ{MODEL_DIR}ï¼‰ï¼Œå°†è·³è¿‡è®­ç»ƒç›´æ¥éƒ¨ç½²')\n",
    "    SKIP_TRAINING = True\n",
    "\n",
    "print(f'\\n  è®­ç»ƒçŠ¶æ€: {\"â­ï¸  è·³è¿‡\" if SKIP_TRAINING else \"âœ… å°†æ‰§è¡Œ\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-step4-title",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 4 / 5 â€” LoRA å¾®è°ƒè®­ç»ƒ\n",
    "\n",
    "> â³ RTX 4090 çº¦ **15-20 åˆ†é’Ÿ**ï¼›æœ‰å·²æœ‰æƒé‡æˆ–æ— æ•°æ®æ—¶è‡ªåŠ¨è·³è¿‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-step4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SKIP_TRAINING:\n",
    "    print('â­ï¸  è·³è¿‡è®­ç»ƒæ­¥éª¤ï¼ˆå·²æœ‰æƒé‡æˆ–æ— è®­ç»ƒæ•°æ®ï¼‰')\n",
    "else:\n",
    "    import json, os, torch\n",
    "    from datasets import Dataset, Audio\n",
    "    from transformers import (\n",
    "        WhisperProcessor, WhisperForConditionalGeneration,\n",
    "        Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "    )\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "\n",
    "    print(f'ğŸ“¦ åŠ è½½åŸºç¡€æ¨¡å‹: {BASE_MODEL}')\n",
    "    processor = WhisperProcessor.from_pretrained(BASE_MODEL, language=LANGUAGE, task='transcribe')\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL)\n",
    "\n",
    "    # RTX 4090ï¼šä¼˜å…ˆä½¿ç”¨ BF16\n",
    "    USE_BF16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    if USE_BF16:\n",
    "        model = model.to(torch.bfloat16)\n",
    "        print('ğŸ® ä½¿ç”¨ BF16 ç²¾åº¦ï¼ˆRTX 4090 åŸç”Ÿæ”¯æŒï¼‰')\n",
    "    else:\n",
    "        print('âš ï¸  BF16 ä¸æ”¯æŒï¼Œå›é€€åˆ° FP16')\n",
    "\n",
    "    # LoRA é…ç½®ï¼ˆé’ˆå¯¹ Whisper æ³¨æ„åŠ›å±‚ï¼‰\n",
    "    lora_config = LoraConfig(\n",
    "        r=16, lora_alpha=32,\n",
    "        target_modules=['q_proj', 'v_proj'],\n",
    "        lora_dropout=0.05, bias='none'\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # åŠ è½½æ•°æ®\n",
    "    print('\\nğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®...')\n",
    "    labels_path = os.path.join(DATA_DIR, 'labels.json')\n",
    "    with open(labels_path) as f:\n",
    "        records = json.load(f)\n",
    "    for r in records:\n",
    "        r['audio'] = os.path.join(DATA_DIR, r['audio'])\n",
    "\n",
    "    dataset = Dataset.from_list(records).cast_column('audio', Audio(sampling_rate=16000))\n",
    "\n",
    "    def preprocess(batch):\n",
    "        audio = batch['audio']['array']\n",
    "        inputs = processor(audio, sampling_rate=16000, return_tensors='pt')\n",
    "        batch['input_features'] = inputs.input_features[0]\n",
    "        batch['labels'] = processor.tokenizer(batch['text']).input_ids\n",
    "        return batch\n",
    "\n",
    "    print('âš™ï¸  é¢„å¤„ç†æ•°æ®...')\n",
    "    dataset = dataset.map(preprocess, remove_columns=['audio', 'text'])\n",
    "    split = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "    # è®­ç»ƒå‚æ•°ï¼ˆRTX 4090 ä¼˜åŒ–ï¼‰\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=MODEL_DIR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_steps=TRAIN_STEPS,\n",
    "        learning_rate=1e-3,\n",
    "        warmup_steps=50,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=100,\n",
    "        save_steps=100,\n",
    "        logging_steps=25,\n",
    "        predict_with_generate=True,\n",
    "        bf16=USE_BF16,\n",
    "        fp16=not USE_BF16,\n",
    "        report_to='none',\n",
    "        dataloader_num_workers=4,\n",
    "        no_cuda=False,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=split['train'],\n",
    "        eval_dataset=split['test'],\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "\n",
    "    print(f'\\nğŸš€ å¼€å§‹è®­ç»ƒï¼batch_size={BATCH_SIZE}, steps={TRAIN_STEPS}')\n",
    "    trainer.train()\n",
    "\n",
    "    print('\\nğŸ’¾ ä¿å­˜ LoRA æƒé‡...')\n",
    "    model.save_pretrained(MODEL_DIR)\n",
    "    processor.save_pretrained(MODEL_DIR)\n",
    "    print(f'âœ… è®­ç»ƒå®Œæˆï¼æƒé‡å·²ä¿å­˜è‡³ {MODEL_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-step5-title",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 5 / 5 â€” å¯åŠ¨ FastAPI æ¨ç†æœåŠ¡\n",
    "\n",
    "> ğŸ’¡ JupyterLab ä¸­æœåŠ¡ä»¥**åå°çº¿ç¨‹**è¿è¡Œï¼Œä¸ä¼šé˜»å¡ Notebook  \n",
    "> è®¿é—® `http://localhost:8000/health` å¯ç¡®è®¤æœåŠ¡çŠ¶æ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-step5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, torch, threading, uvicorn, nest_asyncio, logging\n",
    "import soundfile as sf\n",
    "from fastapi import FastAPI, UploadFile, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from peft import PeftModel\n",
    "\n",
    "# JupyterLab å¿…é¡»ï¼šå…è®¸åµŒå¥—äº‹ä»¶å¾ªç¯\n",
    "nest_asyncio.apply()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('resonance')\n",
    "\n",
    "# â”€â”€ åˆå§‹åŒ– FastAPI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "app = FastAPI(title='Project Resonance ASR', version='1.0.0')\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=['*'],\n",
    "    allow_methods=['POST', 'GET'],\n",
    "    allow_headers=['*'],\n",
    ")\n",
    "\n",
    "# â”€â”€ åŠ è½½æ¨¡å‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...')\n",
    "_processor = WhisperProcessor.from_pretrained(MODEL_DIR)\n",
    "_base = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL)\n",
    "_model = PeftModel.from_pretrained(_base, MODEL_DIR)\n",
    "_model.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    _dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    _model = _model.to('cuda', dtype=_dtype)\n",
    "    print(f'âœ… æ¨¡å‹å·²åŠ è½½è‡³ GPUï¼ˆç²¾åº¦: {_dtype}ï¼‰')\n",
    "else:\n",
    "    print('âš ï¸  GPU ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU æ¨ç†ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰')\n",
    "\n",
    "# â”€â”€ æ¥å£å®šä¹‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@app.get('/health')\n",
    "def health():\n",
    "    info = {'status': 'ok', 'model': BASE_MODEL}\n",
    "    if torch.cuda.is_available():\n",
    "        info.update({\n",
    "            'gpu': torch.cuda.get_device_name(0),\n",
    "            'memory_used_mb': round(torch.cuda.memory_allocated(0) / 1024**2, 1),\n",
    "            'memory_total_mb': round(torch.cuda.get_device_properties(0).total_memory / 1024**2, 1),\n",
    "        })\n",
    "    return info\n",
    "\n",
    "@app.post('/transcribe')\n",
    "async def transcribe(file: UploadFile):\n",
    "    try:\n",
    "        data = await file.read()\n",
    "        audio, sr = sf.read(io.BytesIO(data))\n",
    "        if sr != 16000:\n",
    "            import librosa\n",
    "            audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        inputs = _processor(audio, sampling_rate=16000, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            ids = _model.generate(**inputs, language=LANGUAGE, task='transcribe')\n",
    "        text = _processor.batch_decode(ids, skip_special_tokens=True)[0].strip()\n",
    "        logger.info(f'è¯†åˆ«ç»“æœ: {text}')\n",
    "        return {'text': text, 'success': True}\n",
    "    except Exception as e:\n",
    "        logger.error(f'è¯†åˆ«å¤±è´¥: {e}')\n",
    "        raise HTTPException(500, f'è¯†åˆ«å¤±è´¥: {str(e)}')\n",
    "\n",
    "# â”€â”€ åå°å¯åŠ¨æœåŠ¡ï¼ˆä¸é˜»å¡ Notebookï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "config = uvicorn.Config(app, host='0.0.0.0', port=SERVER_PORT, log_level='info')\n",
    "server = uvicorn.Server(config)\n",
    "\n",
    "thread = threading.Thread(target=server.run, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "print()\n",
    "print('=' * 50)\n",
    "print('  ğŸ‰ æœåŠ¡å·²å¯åŠ¨ï¼')\n",
    "print('=' * 50)\n",
    "print(f'  å¥åº·æ£€æŸ¥:  http://localhost:{SERVER_PORT}/health')\n",
    "print(f'  è¯†åˆ«æ¥å£:  POST http://localhost:{SERVER_PORT}/transcribe')\n",
    "print()\n",
    "print('  åœ¨ App è®¾ç½®ä¸­å¡«å†™å…¬ç½‘åœ°å€ï¼š')\n",
    "print(f'  http://<æœåŠ¡å™¨å…¬ç½‘IP>:{SERVER_PORT}/transcribe')\n",
    "print()\n",
    "print('  åœæ­¢æœåŠ¡ï¼šé‡å¯æ­¤ Kernel æˆ–å…³é—­ JupyterLab å³å¯')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-test-title",
   "metadata": {},
   "source": [
    "## ğŸ§ª å¯é€‰ï¼šæœ¬åœ°å¿«é€Ÿæµ‹è¯•è¯†åˆ«æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# ä¿®æ”¹ä¸ºä½ çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„\n",
    "TEST_AUDIO = '/root/data/test.wav'\n",
    "\n",
    "if not __import__('os').path.isfile(TEST_AUDIO):\n",
    "    print(f'âš ï¸  æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨ï¼š{TEST_AUDIO}')\n",
    "    print('è¯·ä¿®æ”¹ TEST_AUDIO å˜é‡æŒ‡å‘ä¸€ä¸ª WAV éŸ³é¢‘æ–‡ä»¶')\n",
    "else:\n",
    "    with open(TEST_AUDIO, 'rb') as f:\n",
    "        resp = requests.post(\n",
    "            f'http://localhost:{SERVER_PORT}/transcribe',\n",
    "            files={'file': ('test.wav', f, 'audio/wav')}\n",
    "        )\n",
    "    if resp.ok:\n",
    "        result = resp.json()\n",
    "        print(f'âœ… è¯†åˆ«ç»“æœï¼š{result[\"text\"]}')\n",
    "    else:\n",
    "        print(f'âŒ è¯·æ±‚å¤±è´¥ï¼š{resp.status_code} â€” {resp.text}')"
   ]
  }
 ]
}
